---
title: "Project Heart-Disease-Analysis"

author: "Yamileth Hercules 100385215", 
        "Mary Alexandra Garcia 100391387", 
        "Maria Christina Alexandria Perocho 100392385", 
        "Lovella Inso 100392634"
        
date: "2023-03-30"
output: html_document
---


Importing Needed Libraries
```{r}
library(pROC)
library(tidyverse)
library(nnet)
library(ROCR)
library(packHV)
library(plyr)
library(ggplot2)
library("car")
library("ggpubr")
library(interactions)
```

```{r}
library(dplyr)

# Replace missing values with the median for multiple columns
data <- data %>%
  mutate(column1 = ifelse(is.na(column1), median(column1, na.rm = TRUE), column1),
         column2 = ifelse(is.na(column2), median(column2, na.rm = TRUE), column2),
         column3 = ifelse(is.na(column3), median(column3, na.rm = TRUE), column3))
```

```{r}
df<- read.csv('heart_disease.csv')
View(df)
head(df)
```
## 
```{r}
str(df)
```

### Declaring categorical variables as factor

```{r}
df$sex<-as.factor(df$sex)
df$cp<-as.factor(df$cp)
df$cp<-as.factor(df$cp)
df$fbs<-as.factor(df$fbs)
df$restecg<- as.factor(df$restecg)
df$exang<- as.factor(df$exang)
df$slope<-as.factor(df$slope)
df$ca<-as.factor(df$ca)
df$thal<- as.factor(df$thal)
df$HD<-as.factor(df$HD)
```

### Separating numerical variables 
```{r}
num_vars <- sapply(df, is.numeric)
num_df <- df[, num_vars]
head(num_df)
```

# Creating one subset for categorical variables
```{r}
sub <- df[, c("HD", "sex", "cp", "fbs", "restecg", "exang", "slope", "ca", "thal")]
sub
```

## Performing the chi-squared to validate the independence between the response variable and the explanatory variables:

Creating the hypothesis:
Ho: The explanatory variable and the response variable 'HD' are independent to each other.
Ha: The explanatory variable and the response variable 'HD' are not independent to each other.

```{r}
combos <- combn(ncol(sub), 2)

df_list <- alply(combos, 2, function(x) {
  test <- chisq.test(sub[, x[1]], sub[, x[2]])
  
out <- data.frame('row' = colnames(sub)[x[1]]
                    , 'column' = colnames(sub[x[2]])
                    , "Chi.Square" = round(test$statistic,3)
                    ,  "df"= test$parameter
                    ,  "p.value" = test$p.value
                    )
  return(out)

})

result_df <- do.call(rbind, df_list)
head(result_df)
```

Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that the explanatory variable and the response variable 'HD' are not independent to each other

# Plotting the histogram and boxplot to see the distribution of numerical variables
```{r}
for (var in names(num_df)) {
  hist_boxplot(num_df[[var]],col="lightblue",freq=TRUE,  xlab = var,main = paste("Histogram of", var))
} 
```

### Interpretation: 
We can see the distribution and the boxplot of the variables in the above graphs. Now we are going to mention son interpretation of those:
The variable `age` show a normal distribution with no outliers. 
The variable `thalach` follows a normal distribution but we can see some outliers in the boxplot.
The variables that show a skewed distribution and have outliers are : `trestbps`, `chol` and  `oldpeak`. 

##### Note: 
In clinical studies, it is generally not recommended to filter or change the distribution of the data, including removing outliers, because doing so can introduce bias and distort the results of the study. Outliers are data points that are significantly different from the rest of the data and may be due to measurement error, biological variation, or other factors. While outliers can be a inconvenience, they may also contain important information about the underlying population being studied, and removing them can lead to biased estimates and incorrect conclusions.

Therefore, it is generally recommended to report and analyze all data, including outliers, and to use appropriate statistical methods that can handle non-normal distributions or outliers, such as non-parametric tests or robust regression models. This can help ensure that the results of clinical studies are accurate, unbiased, and generalizable to the broader population of interest.

Because of the above mention we are not  going to filter outlier or remove them.

## Correlations between numerical variables 

```{r}
pairs(num_df)
```

```{r}
cor(num_df)
```

The table above show if the variable how a Pearson's correlation. We interpret the results that way:
* 1. Weak correlation: 0 < |r| < 0.3  
* 2. Moderate correlation:  0.3 < |r| < 0.7  
* 3. Strong correlation:   |r| > 0.7

Therefore, the  tabla  above shows that there are not moderate or strong correlation, just shows weak correlation between the numerical variables. 

For example `thalach`and `age` shows a weak negative linear association between them (-0.353)

# Creating a heatmap to verify the correlation result:
```{r}
heatmap(cor(num_df), cmap = colorRampPalette(c("blue", "white", "red"))(100))
```

### Interpretation: 
A heatmap is a graphical representation of data where the values of a matrix are represented as colors. Heatmaps are commonly used to display large datasets and provide a visual summary of the data.

As we can see above this heatmap represent the numerical variables that have a correlation. High correlation are represented by darker color and weak correlation are represented by lighter colors. By examining the heatmap, we could quickly identify patterns in the data and see that there is not strong correlation between the numerical variables.Therefore, with those variables it is quite likely that we are not going to have a multidisciplinary problems. 

## Visualizacion of  Normality distribution 
 
### For the entire variables

```{r}
 for (var in names(num_df)) {
  qqPlot(num_df[[var]], main = paste("Q-Q plot of", var))
}
```
But because we are going to analises two sample populations divided by  the `HD` variable (0, 1). It is important to see if both samples follows a normal distribution, therefore, we are going to do that by analizing the graphs. 

To check the normality distribution of the data, we are going to visualize a Q-Q Plot for every two samples
```{r}
# Normality of the two samples 
 for (var in names(num_df)) {
  group0 <- df[df$HD == 0,][,var]
  group1 <- df[df$HD == 1,][,var]
  par(mfrow=c(1,2)) # Set up 1x2 grid of plots
  qqPlot(group0, main = "QQ-Plot not having HD G-0")
  qqPlot(group1, main = "QQ-Plot having HD G-1")

 }
```

Since the p value is less than 0.05 we reject the null hypothesis and  at 95% percentage of confidence level, we can conclude that there are not equal variances between the two groups. 
We can interpret this result as evidence that the assumption of equal variances for the two groups is  invalid.

### Interpretation:

* For the variable `age` we can see that there are some parts at at the beggining and at the end that the points ar out of the diagonal line. However, in general the points are over the diagonal line. So we can conclude that the variable for the two groups follow a normal distribution.With the aforementioned variable we are going to decide if we are going to use a Anova test or a t-test taking in consideration the assumptions of them. 
* For the variables : `trestbps`, `chol`, `chalach`, and `oldpeak` the points are very outside of the diagonal line so we can conclude that they dont follow a normal distribution, the one that is more evident is oldpeak. For the variables mentioned before we are going to use non parametric tests. We are going to use Main Whitney U test or Wilcoxon test, taking in consideration that violated the assumption of normality distribution. 

# Data Visualization of the two groups 

```{r}
# Create a figure with subplots for each variable
par(mfrow=c(1, ncol(num_df)), mar=c(4, 4, 2, 1), oma=c(0, 0, 2, 0))
for (i in 1:(ncol(num_df))) {
  boxplot(num_df[,i] ~ df$HD, main=names(num_df)[i], ylab="Value")
}

# Add a title and adjust the margins
mtext("Boxplots for Five Numerical Variables by Group", outer=TRUE, cex=1.5)
```

# Analizing `Age` variable as important factor with a T- test. Two sided test.

```{r}
ggboxplot(df, x = "HD", y = "age", 
          color = "HD", palette = c("#00AFBB", "#E7B800"),
        ylab = "age", xlab = "HD")
```
### Test homogeneity of variances

* Ho: The variances of the groups are equal.
* Ha: At least one of the group variances is different from the others.
Significance level(alpha)= 0.05

#### Asumptions for this test:

* Independent observations 
* The test variable is quantitative, that is, not nominal or ordinal.

If any of these assumptions are violated, a different test should be used.

```{r}
library(car)
leveneTest(age ~ HD, data = df)
```

Interpretation:
Since the p-value is less than 0.05, we  reject the null hypothesis and we can say  there is a  difference in the variances of the `age` of the two populations or group. So, we can conclude that the two groups have different variances. 




#### Testing significance of the symmetric distribution of the variable  

* Ho: The two independent samples have equal means 
* Ha: The two independent samples have not equal means

Reject the null hypothesis is less or equal  0.05 significance level.

Assumptions:
    
* Data values must be independent. Measurements for one observation do not affect measurements for any other observation.
* Data in each group must be obtained via a random sample from the population.
* Data in each group are normally distributed.
* Data values are continuous.
* The variances for the two independent groups are equal.

```{r}
# Compute t-test
res <- t.test(age ~ HD, data = df, alternative = "two.sided", var.equal = FALSE)
res
```

#### Interpretation:
* Since the p-value is less than 0.05, we  reject the null hypothesis and we can say there is a  difference in the means between the two populations or group. Therefore, we can expect that the `age` has a statistical significant difference in the means in both groups( 0 and 1). So, we can conclude that this variable is a important factor for the model. 


We will create a function to apply the Wilcoxon test between all the the response variable and the explanatory variables:

## Wilcoxon Test- Non parametric test:

#### Testing significance of the symmetric distribution between the two groups. Two sided test.
* Ho: There is a not difference in the distribution between the two populations or groups
* Ha: There is a significant difference in the distribution between the two population or groups
If p_value is less than 0.05 we reject the null hypothesis

#### Asumptions for this test:
It is an alternative measurement of unpaired t test.A non parametric measurement

* Not normal distributed 
* The measure variable should be continuous or at least a order scale
* Two independent unpaired group

```{r}
test<- function(x) {
  wilcoxon <- wilcox.test(num_df[, x]~ as.numeric(df$HD), data = df)
  
  res <- data.frame('Explanatory' = 'HD'
                    , 'Response' = colnames(num_df)[x]
                    ,  "P.value" =  wilcoxon$p.value
                    )
  return(res)
}
num <- do.call(rbind, lapply(seq_along(num_df)[-1], test))
num
```

#### Interpretation:
* Since the p-value is less than 0.05, we  reject the null hypothesis and we can say there is a  difference in the distribution between the two populations or group. Therefore, we can expect that the variable `trestbps`, `chol`, `thalach` and `oldpeak`  have a statistical significant difference in the distribution in both groups( 0 and 1). So, we can conclude that these variable are a important factor for the model. 
Additional we are just going to try do the same with `trestbps` just for practice even though that the variable does not follow a normal distribution

# Analizing `trestbps` variable as important factor 
```{r}
res1 <-t.test(trestbps ~ HD, data = df, alternative = "two.sided", var.equal = TRUE)
res1 
```
#### Interpretation:
* Since the p-value is less than 0.05, we  reject the null hypothesis and we can say there is a  difference in the means between the two populations or group. Therefore, we can expect that the `trestbps` has a statistical significant difference in the means in both groups( 0 and 1). So, we can conclude that this variable is a important factor for the model. 

### Spliting data into train and test
```{r}
#Splitting in a random way the data
data <-sort(sample(nrow(df), nrow(df)*.8))
train<-df[data,]
test<-df[-data,]
```

# We will create our saturated model 
```{r}
model_1<- glm(HD ~.,family= binomial(link='logit'), data=train)
summary(model_1)
```

The variables that have p-value <0.05 in this saturated model are: `trestbps`, `age`, `sex`, `chol`, `thalach`, `exang` and  `oldpeak`. Those variables are important factor for the model.Next, considering that the p-values of some levels in the variables `cp`, `ca` and `thal` are less than 0.05, we will validate them one by one if they have any effect on the model and on the response variable:

```{r}
model_2<- glm(HD ~ age+sex+trestbps+chol+thalach+exang+oldpeak,family= binomial(link='logit'), data=train)
summary(model_2)
```
# --> Model 2 including the cp variable:
 
```{r}
model_cp<- glm(HD ~ age+trestbps+sex+chol+thalach+exang+oldpeak+cp,family= binomial(link='logit'), data=train)
summary(model_cp)
```

-  Testing if the cp variable is significant for the model

Ho: The base line model (reduced) is appropriate
Ha: The full model (including cp variable) is appropriate

```{r}
pchisq(701.21 - 627.07,726 - 723, lower.tail = FALSE)
```
Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that the full model (including cp variable) is appropriate.

# --> Model 2 including the ca variable:

```{r}
model_ca<- glm(HD ~ age+sex+trestbps+chol++thalach+exang+oldpeak+ca,family= binomial(link='logit'), data=train)
summary(model_ca)
```

-  Testing if the ca variable is significant for the model

Ho: The base line model (reduced) is appropriate
Ha: The full model (including ca variable) is appropriate

```{r}
pchisq(701.21 - 686.67,726 - 723, lower.tail = FALSE)
```

Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that the full model (including ca variable) is appropriate.

# --> Model 2 including the thal variable:

```{r}
model_thal<- glm(HD ~ age+sex+trestbps+chol+thalach+exang+oldpeak+thal,family= binomial(link='logit'), data=train)
summary(model_thal)
```

-  Testing if the thal variable is significant for the model

Ho: The base line model (reduced) is appropriate
Ha: The full model (including thal variable) is appropriate

```{r}
pchisq(701.21 - 683.72,726 - 724, lower.tail = FALSE)
```

Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that the full model (including thal variable) is appropriate.

# --> Validating if our final base line model is better than the saturated model:

```{r}
baseline<- glm(HD ~ age+sex+cp+trestbps+chol+thalach+exang+oldpeak+ca+thal,family= binomial(link='logit'), data=train)
summary(baseline)
```

-  Testing if our base line model (reduced) is better than the saturated model:

Ho: The base line model (reduced) is appropriate
Ha: The saturated model is appropriate

```{r}
pchisq(baseline$deviance- model_1$deviance, baseline$df.residual -model_1$df.residual, lower.tail= FALSE)
```


Since the p value is greater than 0.05, we do not have enough statistical evidence to reject the null hypothesis and we can conclude that out base line model (reduced) is appropriate.

## Check Multicollinearity 
```{r}
vif(baseline)
```
Since the VIF is less than 5 for every variable we do not need to remove any variable from our model.

# Interaction term:

  - Creating the interaction between the variables age and trestbps:
  
```{r}
interact_plot(glm(formula=HD~age+sex+cp+trestbps+chol+thalach+exang+oldpeak+ca+thal+age:trestbps,family=binomial(link=logit),
        data=train),
        pred=age, modx=trestbps, geom ='line')
```

  - Creating the interaction between the variables chol and cp:

```{r}
interact_plot(glm(formula=HD~age+sex+cp+trestbps+chol+thalach+exang+oldpeak+ca+thal+chol:cp,family=binomial(link=logit),
        data=train),
        pred=chol, modx=cp, geom ='line')
```
Based on the plots above, we will evaluate our base line model vs the base line model + the interactions between chol and cp:

```{r}
model_with_interaction<- glm(HD ~ age+sex+cp+trestbps+chol+thalach+exang+oldpeak+ca+thal+age:trestbps,family= binomial(link='logit'), data=train)
summary(model_with_interaction)
```

-  Testing if our base line model (reduced) is better than the model with interactions:

Ho: The reduce model is appropriate(without the interaction)
Ha:  The full model is appropriate(with interaction)

```{r}
pchisq(606.15 - 601.52,717-716,  lower.tail = FALSE)
```
Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that out full model (with interaction) is appropriate.


```{r}
pchisq(baseline$deviance-model_with_interaction$deviance,baseline$df.residual- model_with_interaction$df.residual, lower.tail= FALSE)
```





# ROC Curve based on the 2 models created above.
# PS. this is not the final model to do Classification table and ROC Curve
```{r}
# Model Performance Evaluation
prob <- predict(model_1, type = "response")

prob2 <- predict(model_2, type = "response")

```

```{r}
table <- table(prob,train$HD)
table2 <- table(prob2,train$HD)
```

```{r}
table(train$HD)
```

Calculating a possible threshold for the prediction
```{r}
# max prediction  / total number of rows : based on the table above, value 1 has 405 prediction
thres <- 405/734
thres
```

# Classification table for Model 1
```{r}
# Assign each observation to a class based on the threshold value
predicted_classes <- ifelse(prob >= thres, 1, 0)
actual_classes <- train$HD

# Convert the predicted and actual class vectors to factors
predicted_classes <- factor(predicted_classes, levels = c("0", "1"))
actual_classes <- factor(actual_classes, levels = c("0", "1"))

# Create a table of predicted versus actual classes
tab <- table(predicted_classes, actual_classes)

# Print the table
print(tab)
```
# Classification table for Model 2
```{r}
# Assign each observation to a class based on the threshold value
predicted_classes2 <- ifelse(prob2 >= thres, 1, 0)
actual_classes <- train$HD

# Convert the predicted and actual class vectors to factors
predicted_classes2 <- factor(predicted_classes2, levels = c("0", "1"))
actual_classes2 <- factor(actual_classes, levels = c("0", "1"))

# Create a table of predicted versus actual classes
tab2 <- table(predicted_classes2, actual_classes)

# Print the table
print(tab2)
```

#Correct Classification
```{r}

sum1 <- sum(diag(tab))/sum(tab)
sum2 <- sum(diag(tab2))/sum(tab2)
print(c(CorrectClassification1 = sum1, CorrectClassification2 = sum2))
```
#Misclassification Rate
```{r}
m1 <- 1-sum(diag(tab))/sum(tab)
m2 <- 1-sum(diag(tab2))/sum(tab2)
print(c(MisClassification1 = m1, MisClassification2 = m2))
```
#Prediction Probability
```{r}
pred_val <- prediction(pred_val,train$HD)
pred_val2 <- prediction(pred_val2,train$HD)
```

```{r}
eval <- performance(pred_val,"acc")
eval2 <- performance(pred_val2,"acc")
```


#### Classification Report for Model 1
### Identifying Accuracy and Cut-off, Specificity and Sensitivity
```{r}
# accuracy value for Y
max <- which.max(slot(eval, "y.values")[[1]])
accy <- slot(eval, "y.values")[[1]][max]
```
```{r}
# cut-off value for X
cut <- slot(eval, "x.values")[[1]][max]
```
```{r}
print(c(Accuracy=accy, Cutoff= cut))
```
```{r}
# true positive rate
tpr = 332/(332+73)
# false positive rate
fpr = 64/(64+256)
print(c(TruePR =tpr, FalsePR = fpr ))
```

#### Classification Report for Model 2
### Identifying Accuracy and Cut-off,  Specificity and Sensitivity
```{r}
# accuracy value for Y
max2 <- which.max(slot(eval2, "y.values")[[1]])
accy2 <- slot(eval2, "y.values")[[1]][max2]
```
```{r}
# cut-off value for X
cut <- slot(eval2, "x.values")[[1]][max2]
```
```{r}
print(c(Accuracy=accy2, Cutoff= cut))
```
```{r}
# true positive rate
tpr2 = 310/(310+95)
# false positive rate
fpr2 = 70/(70+259)
print(c(TruePR =tpr2, FalsePR = fpr2 ))
```


# Receiver Operating Characteristic (ROC) Curve
```{r}
roc <- performance(pred_val, "tpr", "fpr")
roc2 <- performance(pred_val2, "tpr", "fpr")
```


```{r}
plot(roc,
     colorize=T,
     main = "ROC Curve",
     ylab = "Sensitivity",
     xlab = "1-Specificity")
plot(roc2, add=TRUE, col='red')
abline(a=0, b=1)
```



# Results 
10, 
11, 
12, 
